{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD: concept\n",
      "WORD: part\n",
      "WORD: equality\n",
      "WORD: involved\n",
      "WORD: fairness\n",
      "WORD: the\n",
      "concept.n.01\t\t['idea.n.01']\n",
      "part.n.01\t\t['relation.n.01']\n",
      "part.n.02\t\t['object.n.01']\n",
      "part.n.03\t\t['thing.n.12']\n",
      "part.n.04\t\t['concern.n.01']\n",
      "region.n.01\t\t['location.n.01']\n",
      "function.n.03\t\t['duty.n.02']\n",
      "character.n.04\t\t['portrayal.n.02']\n",
      "share.n.01\t\t['assets.n.01']\n",
      "part.n.09\t\t['concept.n.01']\n",
      "part.n.10\t\t['line.n.11']\n",
      "part.n.11\t\t['tune.n.01']\n",
      "contribution.n.01\t\t['attempt.n.01']\n",
      "separate.v.09\t\t['move.v.03']\n",
      "separate.v.08\t\t[]\n",
      "depart.v.03\t\t['leave.v.01']\n",
      "separate.v.12\t\t['change.v.02']\n",
      "separate.v.02\t\t['move.v.02']\n",
      "partially.r.01\t\t[]\n",
      "equality.n.01\t\t['sameness.n.01']\n",
      "equality.n.02\t\t['status.n.01']\n",
      "involve.v.01\t\t['refer.v.02']\n",
      "involve.v.02\t\t['admit.v.03']\n",
      "imply.v.05\t\t['have.v.02']\n",
      "necessitate.v.01\t\t[]\n",
      "involve.v.05\t\t['include.v.01']\n",
      "involve.v.06\t\t['absorb.v.09']\n",
      "involve.v.07\t\t['complicate.v.02']\n",
      "involved.a.01\t\t[]\n",
      "involved.s.02\t\t[]\n",
      "involved.s.03\t\t[]\n",
      "byzantine.s.03\t\t[]\n",
      "involved.s.05\t\t[]\n",
      "fairness.n.01\t\t['justice.n.01']\n",
      "fairness.n.02\t\t['impartiality.n.01']\n",
      "paleness.n.02\t\t['complexion.n.01']\n",
      "comeliness.n.01\t\t['beauty.n.01']\n",
      "\n",
      "\n",
      "concept.n.01\t\t['abstraction.n.01', 'category.n.02', 'conceptualization.n.02', 'fact.n.04', 'hypothesis.n.02', 'law.n.03', 'law.n.04', 'lexicalized_concept.n.01', 'notion.n.02', 'part.n.09', 'property.n.04', 'quantity.n.03', 'rule.n.01', 'rule.n.04', 'whole.n.01']\n",
      "part.n.01\t\t['basis.n.03', 'butt.n.02', 'detail.n.02', 'item.n.01', 'language_unit.n.01', 'member.n.02', 'remainder.n.01', 'subpart.n.01', 'substance.n.01', 'unit.n.02']\n",
      "part.n.02\t\t['appendage.n.03', 'bit.n.10', 'bottleneck.n.02', 'bulb.n.03', 'butt.n.09', 'component.n.03', 'cutout.n.03', 'foible.n.02', 'fore_edge.n.01', 'forte.n.03', 'fraction.n.02', 'heel.n.06', 'hub.n.01', 'jetsam.n.01', 'limb.n.04', 'neck.n.04', 'peen.n.01', 'piece.n.01', 'pressing.n.02', 'seat.n.08', 'section.n.04', 'shank.n.06', 'spine.n.04', 'toe.n.04', 'turnout.n.02', 'upstage.n.01', 'upstairs.n.01', 'wreckage.n.01']\n",
      "part.n.03\t\t['acicula.n.01', 'base.n.04', 'body_part.n.01', 'corner.n.09', 'corpus.n.03', 'craton.n.01', 'cutting.n.04', 'fragment.n.01', 'hunk.n.02', 'nub.n.02', 'segment.n.02', 'slice.n.05', 'strip.n.01', 'world.n.06']\n",
      "part.n.04\t\t[]\n",
      "region.n.01\t\t['aerospace.n.01', 'air.n.02', 'atmosphere.n.03', 'belt.n.03', 'biosphere.n.01', 'black_hole.n.01', 'bottom.n.02', 'county.n.01', 'deep_space.n.01', 'depth.n.03', 'distance.n.02', 'eden.n.01', 'extremity.n.04', 'heliosphere.n.01', 'hell.n.01', 'inside.n.01', 'intergalactic_space.n.01', 'interplanetary_space.n.01', 'interstellar_space.n.01', 'ionosphere.n.01', 'kuiper_belt.n.01', 'layer.n.02', 'mare.n.02', 'outside.n.01', 'papua.n.01', 'radius.n.03', 'side.n.01', 'sign_of_the_zodiac.n.01', 'sind.n.01', 'top.n.01', 'vacuum.n.03', 'zodiac.n.01', 'zone.n.03']\n",
      "function.n.03\t\t['capacity.n.05', 'hat.n.02', 'portfolio.n.04', 'second_fiddle.n.02', 'stead.n.01']\n",
      "character.n.04\t\t['bit_part.n.01', 'heavy.n.02', 'hero.n.02', 'heroine.n.01', 'ingenue.n.03', 'title_role.n.01', 'villain.n.02']\n",
      "share.n.01\t\t['allotment.n.01', 'allowance.n.01', 'cut.n.01', 'dispensation.n.02', 'dole.n.01', 'interest.n.05', 'profit_sharing.n.01', 'ration.n.02', 'slice.n.01', 'split.n.03', 'tranche.n.01', 'way.n.12']\n",
      "part.n.09\t\t['beginning.n.03', 'chukker.n.01', 'component.n.01', 'end.n.05', 'final_period.n.01', 'first_period.n.01', 'frame.n.11', 'game.n.05', 'half.n.02', 'high_point.n.01', 'inning.n.01', 'middle.n.02', 'over.n.01', 'period.n.03', 'quarter.n.03', 'second_period.n.01', 'turn.n.09']\n",
      "part.n.10\t\t[]\n",
      "part.n.11\t\t['accompaniment.n.02', 'bass.n.02', 'primo.n.01', 'secondo.n.01', 'voice_part.n.01']\n",
      "contribution.n.01\t\t['end.n.11']\n",
      "separate.v.09\t\t['break_up.v.03', 'diffract.v.01', 'disperse.v.04']\n",
      "separate.v.08\t\t['break.v.18', 'break_with.v.01', 'disassociate.v.01', 'disunify.v.01', 'divorce.v.02', 'give_the_axe.v.02', 'secede.v.01']\n",
      "depart.v.03\t\t['blaze.v.04', 'roar_off.v.01', 'sally_forth.v.01', 'take_off.v.03']\n",
      "separate.v.12\t\t['calve.v.01', 'chip.v.01', 'detach.v.03', 'disjoin.v.02', 'dismember.v.02', 'gerrymander.v.01', 'partition.v.01', 'polarize.v.03', 'reduce.v.13', 'segment.v.01', 'segment.v.02', 'segregate.v.02', 'subdivide.v.01']\n",
      "separate.v.02\t\t['break.v.41', 'compartmentalize.v.01', 'cut.v.01', 'disconnect.v.02', 'disjoin.v.01', 'disjoint.v.02', 'gin.v.01', 'joint.v.04', 'polarize.v.02', 'sequester.v.05', 'sever.v.01', 'tear.v.01', 'tear.v.02']\n",
      "partially.r.01\t\t[]\n",
      "equality.n.01\t\t['balance.n.02', 'equatability.n.01', 'equivalence.n.02', 'evenness.n.03', 'isometry.n.04']\n",
      "equality.n.02\t\t['egality.n.01', 'tie.n.03']\n",
      "involve.v.01\t\t['implicate.v.01']\n",
      "involve.v.02\t\t['embroil.v.01', 'entangle.v.01']\n",
      "imply.v.05\t\t['carry.v.10']\n",
      "necessitate.v.01\t\t['claim.v.05', 'compel.v.02', 'cost.v.02', 'cry_out_for.v.01', 'draw.v.22', 'govern.v.04']\n",
      "involve.v.05\t\t[]\n",
      "involve.v.06\t\t[]\n",
      "involve.v.07\t\t[]\n",
      "involved.a.01\t\t[]\n",
      "involved.s.02\t\t[]\n",
      "involved.s.03\t\t[]\n",
      "byzantine.s.03\t\t[]\n",
      "involved.s.05\t\t[]\n",
      "fairness.n.01\t\t['non-discrimination.n.01', 'sportsmanship.n.01']\n",
      "fairness.n.02\t\t[]\n",
      "paleness.n.02\t\t[]\n",
      "comeliness.n.01\t\t[]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def lesk(word, sentence):\n",
    "    \"\"\"\n",
    "    Lesk's algoritm implementation. Given a word and a sentence in which it appears,\n",
    "    it returns the best sense of the word.\n",
    "\n",
    "    :param word: word to disabiguate\n",
    "    :param sentence: sentence to compare\n",
    "    :return: best sense of word\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculating the synset of the given word inside WN\n",
    "    word_senses = wn.synsets(word)\n",
    "    best_sense = word_senses[0]\n",
    "    max_overlap = 0\n",
    "\n",
    "    # I choose the bag of words approach\n",
    "    context = bag_of_word(sentence)\n",
    "\n",
    "    for sense in word_senses:\n",
    "        # set of words in the gloss\n",
    "        signature = bag_of_word(sense.definition())\n",
    "\n",
    "        # and examples of the given sense\n",
    "        examples = sense.examples()\n",
    "        for ex in examples:\n",
    "            # after this line, signature will contain for all the words, their\n",
    "            # bag of words definition and their examples\n",
    "            signature = signature.union(bag_of_word(ex))\n",
    "\n",
    "        overlap = compute_overlap(signature, context)\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_sense = sense\n",
    "\n",
    "    return best_sense\n",
    "\n",
    "\n",
    "def bag_of_word(sent):\n",
    "    \"\"\"\n",
    "    Auxiliary function for the Lesk algorithm. Transforms the given sentence\n",
    "    according to the bag of words approach, apply lemmatization, stop words\n",
    "    and punctuation removal.\n",
    "\n",
    "    :param sent: sentence\n",
    "    :return: bag of words\n",
    "    \"\"\"\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punct = {',', ';', '(', ')', '{', '}', ':', '?', '!'}\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(sent)\n",
    "    tokens = list(filter(lambda x: x not in stop_words and x not in punct, tokens))\n",
    "    return set(wnl.lemmatize(t) for t in tokens)\n",
    "\n",
    "\n",
    "def compute_overlap(signature, context):\n",
    "    \"\"\"\n",
    "    Auxiliary function for the Lesk algorithm. Computes the number of words in\n",
    "    common between signature and context.\n",
    "\n",
    "    :param signature: bag of words of the signature (e.g. definitions + examples)\n",
    "    :param context: bag of words of the context (e.g. sentence)\n",
    "    :return: number of elements in commons\n",
    "    \"\"\"\n",
    "\n",
    "    return len(signature & context)\n",
    "\n",
    "\n",
    "def preprocess(synset):\n",
    "    \"\"\"\n",
    "    It does some preprocess: removes the stopword, punctuation and does the\n",
    "    lemmatization of the tokens inside the sentence.\n",
    "    :param definition: a string representing a definition\n",
    "    :return: a set of string which contains the preprocessed string tokens.\n",
    "    \"\"\"\n",
    "    pre_synset = synset.split(\".\")\n",
    "    clean_synset = pre_synset[0]\n",
    "    return clean_synset\n",
    "\n",
    "\n",
    "t = {'fairness', 'concept', 'equality', 'the', 'involved', 'part'}  # {'correct', 'condition', 'morally'}, {'concept', 'refers', 'abstract', 'right'}\n",
    "\n",
    "hyper = {}\n",
    "hypo = {}\n",
    "common = {}\n",
    "\n",
    "for word in t:\n",
    "    print(\"WORD: {}\".format(word))\n",
    "    synsets = wn.synsets(word)\n",
    "\n",
    "    for s in synsets:\n",
    "#         print(\"SYNSET: {}, DEF: {}\".format(s.name(), s.definition()))\n",
    "#         print(\"HYPERONIMS: {}\".format(s.hypernyms()))\n",
    "#         print(\"HYPONIMS: {}\\n\".format(s.hyponyms()))\n",
    "        \n",
    "        if not s.name() in hyper:\n",
    "            temp = []\n",
    "            for x in s.hypernyms():\n",
    "                temp.append(x.name())\n",
    "            hyper[s.name()] = temp\n",
    "            \n",
    "        if not s.name() in hypo:\n",
    "            temp2 = []\n",
    "            for y in s.hyponyms():\n",
    "                temp2.append(y.name())\n",
    "            hypo[s.name()] = temp2\n",
    "\n",
    "#         for h1 in s.hypernyms():\n",
    "#             key = h1.name()\n",
    "#             if not key in hyper:\n",
    "#                 hyper[key] = 1\n",
    "#             else:\n",
    "#                 hyper[key] += 1\n",
    "\n",
    "#         for h2 in s.hyponyms():\n",
    "#             key = h2.name()\n",
    "#             if not key in hypo:\n",
    "#                 hypo[key] = 1\n",
    "#             else:\n",
    "#                 hypo[key] += 1\n",
    "\n",
    "# final_test = {}\n",
    "\n",
    "for elem in hyper:\n",
    "    print(\"{}\\t\\t{}\".format(elem, hyper[elem]))\n",
    "    \n",
    "print(\"\\n\")\n",
    "\n",
    "for elem2 in hypo:\n",
    "    print(\"{}\\t\\t{}\".format(elem2, hypo[elem2]))\n",
    "    \n",
    "# #print(wn.synset(\"relation.n.01\"))\n",
    "\n",
    "# sentence = \"the concept of fairness, equality for all the parts involved\"\n",
    "# for h3 in hyper:\n",
    "#     final_test[h3] = lesk(preprocess(h3), sentence)\n",
    "    \n",
    "    \n",
    "# print(final_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a dramatic work intended for performance by actors on a stage'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset(\"play.n.01\").definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
