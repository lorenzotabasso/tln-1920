{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.wsd import lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPERONIMI\n",
    "syns = wn.synsets(\"thing\")\n",
    "names = []\n",
    "\n",
    "for i, s in enumerate(syns, start=0):\n",
    "    hyper = lambda s: s.hypernyms()  # SOPRA-NOME, categoria superiore della parola\n",
    "    temp = list(s.closure(hyper, depth=3))\n",
    "    names.extend([x.name().split(\".\")[0] for x in temp])\n",
    "    #print(\"SYN: {} \\t HYPER: {}\".format(s,t))\n",
    "\n",
    "print(names)\n",
    "#print(len(names))\n",
    "#print(set(names))\n",
    "#print(len(set(names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPONIMI\n",
    "syns = wn.synsets(\"justice\")\n",
    "names = []\n",
    "\n",
    "for i, s in enumerate(syns, start=0):\n",
    "    hyper = lambda s: s.hyponyms()  # SOTTONOME, significato semantico incluso in altra parola\n",
    "    temp = list(s.closure(hyper, depth=1))\n",
    "    names.extend([x.name().split(\".\")[0] for x in temp])\n",
    "    #print(\"SYN: {} \\t HYPER: {}\".format(s,t))\n",
    "\n",
    "print(names)\n",
    "#print(len(names))\n",
    "#print(set(names))\n",
    "#print(len(set(names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset(\"justice.n.01\").definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = wn.synset(\"apple.n.01\").hypernyms()\n",
    "for v in h:\n",
    "    print(v.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset(\"pome.n.01\").definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    It reads che definition's CSV\n",
    "    :return: four list containing the read definitions.\n",
    "    \"\"\"\n",
    "    with open(options[\"output\"] + 'content-to-form.csv', \"r\", encoding=\"utf-8\") as content:\n",
    "        cnt = csv.reader(content, delimiter=';')\n",
    "\n",
    "        dictionary = {}\n",
    "        i = 0\n",
    "        for line in cnt:\n",
    "            dictionary[i] = line\n",
    "            i += 1\n",
    "\n",
    "        return dictionary\n",
    "\n",
    "\n",
    "def preprocess(definition):\n",
    "    \"\"\"\n",
    "    It does some preprocess: removes stopwords, punctuation and does the\n",
    "    lemmatization of the tokens inside the sentence.\n",
    "    :param definition: a string representing a definition\n",
    "    :return: a set of string which contains the preprocessed string tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    # Removing stopwords\n",
    "    definition = definition.lower()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punct = {',', ';', '(', ')', '{', '}', ':', '?', '!', '.'}\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(definition)\n",
    "    tokens = list(filter(lambda x: x not in stop_words and x not in punct, tokens))\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatized_tokens = set(wnl.lemmatize(t) for t in tokens)\n",
    "\n",
    "    return lemmatized_tokens\n",
    "\n",
    "\n",
    "def preprocess_synset(synset):\n",
    "    \"\"\"\n",
    "    It does some preprocess: removes the stopword, punctuation and does the\n",
    "    lemmatization of the tokens inside the sentence.\n",
    "    :param definition: a string representing a definition\n",
    "    :return: a set of string which contains the preprocessed string tokens.\n",
    "    \"\"\"\n",
    "    pre_synset = synset.split(\".\")\n",
    "    clean_synset = pre_synset[0]\n",
    "    return clean_synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "        \"output\": \"/Users/lorenzotabasso/Desktop/University/TLN/Progetto/19-20/tln-1920/part3/exercise2/input/\",\n",
    "    }\n",
    "\n",
    "content = load_data()  # Loading the content-to-form.csv file\n",
    "\n",
    "'''\n",
    "1. prendo definzione, disambiguo con pos-tagging. il primo nome è il genus\n",
    "2. come approccio personalizzato, teveno un dizionario di genus (dopo aver esplorato tutte le definizioni) e espandevo solo il genus più frequente\n",
    "riducendo la ricerca\n",
    "3. prendo da wordnet i synsets di quel sostantivo, e per ognuno di essi parto in basso con gli iponimi\n",
    "4. calcolo l'iponimo dell'iponimo dell'iponimo..., per non sclerare utilizza la closure (chiusura trasitiva). Calcolo gli iponimi fino a un certo \n",
    "livello\n",
    "5. calcola iponimo con più overlapping, stili classifica\n",
    "'''\n",
    "\n",
    "for index in content:\n",
    "#for index in range(1):\n",
    "    \n",
    "    hyponyms_list = []\n",
    "    \n",
    "    for definition in content[index]:\n",
    "    #for definition in content[0]:\n",
    "        genus_dict = {}\n",
    "        hyponyms = []\n",
    "            \n",
    "        def_tokens = word_tokenize(definition)\n",
    "        results = nltk.pos_tag(def_tokens)\n",
    "        \n",
    "        possibles_genus = list(filter(lambda x: x[1] == \"NN\", results))\n",
    "        # Es.: [('abstract', 'NN'), ('concept', 'NN'), ('idea', 'NN'), ('fairness', 'NN'), ('front', 'NN'), ('code', 'NN'), ('community', 'NN')]\n",
    "\n",
    "        for g in possibles_genus:\n",
    "            if not g[0] in genus_dict:\n",
    "                genus_dict[g[0]] = 1\n",
    "            else:\n",
    "                genus_dict[g[0]] += 1\n",
    "    \n",
    "#         print(index, genus)\n",
    "#         print(\"{} - {}\\n\".format(index, genus_dict))\n",
    "        \n",
    "        if len(genus_dict) > 0:\n",
    "            genus = max(genus_dict, key=genus_dict.get)\n",
    "#             print(\"GENUS: \" + genus)\n",
    "        \n",
    "            syns = wn.synsets(genus)\n",
    "        \n",
    "            # Prendiamo tutti gli iponimi per il genus della singola definizione\n",
    "            for i, s in enumerate(syns, start=0):\n",
    "                hypon = lambda s: s.hyponyms()  # SOTTONOME, significato semantico incluso in altra parola\n",
    "                all_hypon = list(s.closure(hypon, depth=1))  # TODO: aumentare a 2,3\n",
    "                hyponyms.extend([x.name().split(\".\")[0] for x in all_hypon])\n",
    "#                 print(\"SYN: {} \\t HYPER: {}\".format(s,t))\n",
    "\n",
    "#             print(index, hyponyms, \"\\n\")\n",
    "#         else:\n",
    "#             print(\"NADA\")\n",
    "            \n",
    "        \n",
    "        hyponyms_list.append(' '.join(hyponyms))\n",
    "    \n",
    "#     print(hyponyms_list)\n",
    "\n",
    "        \n",
    "    '''\n",
    "    CountVectorizer will create k vectors in n-dimensional space, where:\n",
    "    - k is the number of sentences,\n",
    "    - n is the number of unique words in all sentences combined.\n",
    "    If a sentence contains a certain word, the value will be 1 and 0 otherwise\n",
    "    '''\n",
    "    \n",
    "    vectorizer = CountVectorizer()\n",
    "    matrix = vectorizer.fit_transform(hyponyms_list)\n",
    "    \n",
    "    feature_list = vectorizer.get_feature_names()\n",
    "    vectors = matrix.toarray()\n",
    "    \n",
    "    m = vectors.sum(axis=0).argmax()\n",
    "    \n",
    "    print(m)\n",
    "    print(feature_list[m] + '\\n')\n",
    "#     print(feature_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1657\n",
      "rule\n",
      "\n",
      "1657\n",
      "rule\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-176-5839b895a992>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyponyms_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mfeature_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1220\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Da NOMI a IPERONIMI\n",
    "\n",
    "options = {\n",
    "        \"output\": \"/Users/lorenzotabasso/Desktop/University/TLN/Progetto/19-20/tln-1920/part3/exercise2/input/\",\n",
    "    }\n",
    "\n",
    "content = load_data()  # Loading the content-to-form.csv file\n",
    "\n",
    "'''\n",
    "1. prendo definzione, disambiguo con pos-tagging. il primo nome è il genus\n",
    "2. come approccio personalizzato, teveno un dizionario di genus (dopo aver esplorato tutte le definizioni) e espandevo solo il genus più frequente\n",
    "riducendo la ricerca\n",
    "3. prendo da wordnet i synsets di quel sostantivo, e per ognuno di essi parto in basso con gli iponimi\n",
    "4. calcolo l'iponimo dell'iponimo dell'iponimo..., per non sclerare utilizza la closure (chiusura trasitiva). Calcolo gli iponimi fino a un certo \n",
    "livello\n",
    "5. calcola iponimo con più overlapping, stili classifica\n",
    "'''\n",
    "\n",
    "for index in content:\n",
    "#for index in range(1):\n",
    "    \n",
    "    genus_dict = {}\n",
    "    \n",
    "    for definition in content[index]:\n",
    "    #for definition in content[0]:\n",
    "\n",
    "        hypernyms = []\n",
    "        clean_tokens = preprocess(definition)\n",
    "        \n",
    "        all_synsets = []\n",
    "        for word in clean_tokens:\n",
    "            syn = [lesk(definition, word)] # TODO: disambiguare le parole della definizione con lesk e usare i loro synsets per trovare gli iperonimi!\n",
    "            if len(syn) > 0:\n",
    "                for s in syn:\n",
    "                    if s:\n",
    "                        hyper = lambda s: s.hypernyms()\n",
    "                        all_hyper = list(s.closure(hyper, depth=2))  # TODO: aumentare a 2,3\n",
    "                        hypernyms.extend([x.name().split(\".\")[0] for x in all_hyper])\n",
    "\n",
    "                for g in hypernyms:\n",
    "                    if not g in genus_dict:\n",
    "                        genus_dict[g] = 1\n",
    "                    else:\n",
    "                        genus_dict[g] += 1\n",
    "                \n",
    "        # ------------------------------\n",
    "        \n",
    "#         print(genus_dict)\n",
    "    \n",
    "        if len(genus_dict) > 0:\n",
    "            genus = max(genus_dict, key=genus_dict.get)\n",
    "#             print(\"\\n{}\\n\".format(genus))\n",
    "\n",
    "            syns = wn.synsets(genus)\n",
    "\n",
    "            # Prendiamo tutti gli iponimi per il genus della singola definizione\n",
    "            for i, s in enumerate(syns, start=0):\n",
    "                hypon = lambda s: s.hyponyms()  # SOTTONOME, significato semantico incluso in altra parola\n",
    "                all_hypon = list(s.closure(hypon, depth=1))  # TODO: aumentare a 2,3\n",
    "                hyponyms.extend([x.name().split(\".\")[0] for x in all_hypon])\n",
    "#             print(\"SYN: {} \\t HYPER: {}\".format(s,t))\n",
    "#             print(index, hyponyms, \"\\n\")\n",
    "#         else:\n",
    "#             print(\"NADA\")\n",
    "\n",
    "        hyponyms_list.append(' '.join(hyponyms))\n",
    "\n",
    "#         print(hyponyms_list)\n",
    "\n",
    "\n",
    "    '''\n",
    "    CountVectorizer will create k vectors in n-dimensional space, where:\n",
    "    - k is the number of sentences,\n",
    "    - n is the number of unique words in all sentences combined.\n",
    "    If a sentence contains a certain word, the value will be 1 and 0 otherwise\n",
    "    '''\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    matrix = vectorizer.fit_transform(hyponyms_list)\n",
    "\n",
    "    feature_list = vectorizer.get_feature_names()\n",
    "    vectors = matrix.toarray()\n",
    "\n",
    "    m = vectors.sum(axis=0).argmax()\n",
    "\n",
    "    print(m)\n",
    "    print(feature_list[m] + '\\n')\n",
    "#     print(feature_list)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
