{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def lesk(word, sentence):\n",
    "    \"\"\"\n",
    "    Lesk's algoritm implementation. Given a word and a sentence in which it appears,\n",
    "    it returns the best sense of the word.\n",
    "\n",
    "    :param word: word to disabiguate\n",
    "    :param sentence: sentence to compare\n",
    "    :return: best sense of word\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculating the synset of the given word inside WN\n",
    "    word_senses = wn.synsets(word)\n",
    "    best_sense = word_senses[0]\n",
    "    max_overlap = 0\n",
    "\n",
    "    # I choose the bag of words approach\n",
    "    context = bag_of_word(sentence)\n",
    "\n",
    "    for sense in word_senses:\n",
    "        # set of words in the gloss\n",
    "        signature = bag_of_word(sense.definition())\n",
    "\n",
    "        # and examples of the given sense\n",
    "        examples = sense.examples()\n",
    "        for ex in examples:\n",
    "            # after this line, signature will contain for all the words, their\n",
    "            # bag of words definition and their examples\n",
    "            signature = signature.union(bag_of_word(ex))\n",
    "\n",
    "        overlap = compute_overlap(signature, context)\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_sense = sense\n",
    "\n",
    "    return best_sense\n",
    "\n",
    "\n",
    "def bag_of_word(sent):\n",
    "    \"\"\"\n",
    "    Auxiliary function for the Lesk algorithm. Transforms the given sentence\n",
    "    according to the bag of words approach, apply lemmatization, stop words\n",
    "    and punctuation removal.\n",
    "\n",
    "    :param sent: sentence\n",
    "    :return: bag of words\n",
    "    \"\"\"\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punct = {',', ';', '(', ')', '{', '}', ':', '?', '!'}\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(sent)\n",
    "    tokens = list(filter(lambda x: x not in stop_words and x not in punct, tokens))\n",
    "    return set(wnl.lemmatize(t) for t in tokens)\n",
    "\n",
    "\n",
    "def compute_overlap(signature, context):\n",
    "    \"\"\"\n",
    "    Auxiliary function for the Lesk algorithm. Computes the number of words in\n",
    "    common between signature and context.\n",
    "\n",
    "    :param signature: bag of words of the signature (e.g. definitions + examples)\n",
    "    :param context: bag of words of the context (e.g. sentence)\n",
    "    :return: number of elements in commons\n",
    "    \"\"\"\n",
    "\n",
    "    return len(signature & context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPERONIMI\n",
    "sentence = \"the concept of fairness, equality for all the parts involved\"\n",
    "\n",
    "syns = wn.synsets(\"apple\")\n",
    "names = []\n",
    "\n",
    "for i, s in enumerate(syns, start=0):\n",
    "    hyper = lambda s: s.hypernyms()  # SOPRA-NOME, categoria superiore della parola\n",
    "    temp = list(s.closure(hyper, depth=1))\n",
    "    names.extend([x.name().split(\".\")[0] for x in temp])\n",
    "    #print(\"SYN: {} \\t HYPER: {}\".format(s,t))\n",
    "\n",
    "print(names)\n",
    "#print(len(names))\n",
    "#print(set(names))\n",
    "#print(len(set(names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abstraction', 'category', 'conceptualization', 'fact', 'hypothesis', 'law', 'law', 'lexicalized_concept', 'notion', 'part', 'property', 'quantity', 'rule', 'rule', 'whole']\n"
     ]
    }
   ],
   "source": [
    "# IPONIMI\n",
    "syns = wn.synsets(\"concept\")\n",
    "names = []\n",
    "\n",
    "for i, s in enumerate(syns, start=0):\n",
    "    hyper = lambda s: s.hyponyms()  # SOTTONOME, significato semantico incluso in altra parola\n",
    "    temp = list(s.closure(hyper, depth=1))\n",
    "    names.extend([x.name().split(\".\")[0] for x in temp])\n",
    "    #print(\"SYN: {} \\t HYPER: {}\".format(s,t))\n",
    "\n",
    "print(names)\n",
    "#print(len(names))\n",
    "#print(set(names))\n",
    "#print(len(set(names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset(\"apple.n.01\").definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = wn.synset(\"apple.n.01\").hypernyms()\n",
    "for v in h:\n",
    "    print(v.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset(\"pome.n.01\").definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    It reads che definition's CSV\n",
    "    :return: four list containing the read definitions.\n",
    "    \"\"\"\n",
    "    with open(options[\"output\"] + 'content-to-form.csv', \"r\", encoding=\"utf-8\") as content:\n",
    "        cnt = csv.reader(content, delimiter=';')\n",
    "\n",
    "        dictionary = {}\n",
    "        i = 0\n",
    "        for line in cnt:\n",
    "            dictionary[i] = line\n",
    "            i += 1\n",
    "\n",
    "        return dictionary\n",
    "\n",
    "\n",
    "def preprocess(definition):\n",
    "    \"\"\"\n",
    "    It does some preprocess: removes stopwords, punctuation and does the\n",
    "    lemmatization of the tokens inside the sentence.\n",
    "    :param definition: a string representing a definition\n",
    "    :return: a set of string which contains the preprocessed string tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    # Removing stopwords\n",
    "    definition = definition.lower()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punct = {',', ';', '(', ')', '{', '}', ':', '?', '!', '.'}\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(definition)\n",
    "    tokens = list(filter(lambda x: x not in stop_words and x not in punct, tokens))\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatized_tokens = set(wnl.lemmatize(t) for t in tokens)\n",
    "\n",
    "    return lemmatized_tokens\n",
    "\n",
    "\n",
    "def preprocess_synset(synset):\n",
    "    \"\"\"\n",
    "    It does some preprocess: removes the stopword, punctuation and does the\n",
    "    lemmatization of the tokens inside the sentence.\n",
    "    :param definition: a string representing a definition\n",
    "    :return: a set of string which contains the preprocessed string tokens.\n",
    "    \"\"\"\n",
    "    pre_synset = synset.split(\".\")\n",
    "    clean_synset = pre_synset[0]\n",
    "    return clean_synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41213\n",
      "s_law\n",
      "\n",
      "937\n",
      "logic\n",
      "\n",
      "1640\n",
      "wish\n",
      "\n",
      "42557\n",
      "self\n",
      "\n",
      "7326\n",
      "spot\n",
      "\n",
      "11448\n",
      "in\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-282-5e557453abe5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mhypon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyponyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# SOTTONOME, significato semantico incluso in altra parola\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mall_hypon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# TODO: aumentare a 2,3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                 \u001b[0mhyponyms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_hypon\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m#                 print(\"SYN: {} \\t HYPER: {}\".format(s,t))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(self, rel, depth)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msynset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbreadth_first\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msynset_offsets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m                     \u001b[0msynset_offsets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "options = {\n",
    "        \"output\": \"/Users/lorenzotabasso/Desktop/University/TLN/Progetto/19-20/tln-1920/part3/exercise2/input/\",\n",
    "    }\n",
    "\n",
    "content = load_data()  # Loading the content-to-form.csv file\n",
    "\n",
    "'''\n",
    "1. prendo definzione, disambiguo con pos-tagging. il primo nome è il genus\n",
    "2. come approccio personalizzato, teveno un dizionario di genus (dopo aver esplorato tutte le definizioni) e espandevo solo il genus più frequente\n",
    "riducendo la ricerca\n",
    "3. prendo da wordnet i synsets di quel sostantivo, e per ognuno di essi parto in basso con gli iponimi\n",
    "4. calcolo l'iponimo dell'iponimo dell'iponimo..., per non sclerare utilizza la closure (chiusura trasitiva). Calcolo gli iponimi fino a un certo \n",
    "livello\n",
    "5. calcola iponimo con più overlapping, stili classifica\n",
    "'''\n",
    "\n",
    "for index in content:\n",
    "#for index in range(1):\n",
    "    \n",
    "    hyponyms_list = []\n",
    "    \n",
    "    for definition in content[index]:\n",
    "    #for definition in content[0]:\n",
    "        local_genus = {}\n",
    "        hyponyms = []\n",
    "            \n",
    "        def_tokens = word_tokenize(definition)\n",
    "        results = nltk.pos_tag(def_tokens)\n",
    "        \n",
    "        possibles_genus = list(filter(lambda x: x[1] == \"NN\", results))\n",
    "        # Es.: [('abstract', 'NN'), ('concept', 'NN'), ('idea', 'NN'), ('fairness', 'NN'), ('front', 'NN'), ('code', 'NN'), ('community', 'NN')]\n",
    "\n",
    "        for g in possibles_genus:\n",
    "            if not g[0] in local_genus:\n",
    "                local_genus[g[0]] = 1\n",
    "            else:\n",
    "                local_genus[g[0]] += 1\n",
    "    \n",
    "#         print(index, genus)\n",
    "#         print(\"{} - {}\\n\".format(index, local_genus))\n",
    "        \n",
    "        if len(local_genus) > 0:\n",
    "            genus = max(local_genus, key=local_genus.get)\n",
    "#             print(\"GENUS: \" + genus)\n",
    "        \n",
    "            syns = wn.synsets(genus)\n",
    "        \n",
    "            # Prendiamo tutti gli iponimi per il genus della singola definizione\n",
    "            for i, s in enumerate(syns, start=0):\n",
    "                hypon = lambda s: s.hyponyms()  # SOTTONOME, significato semantico incluso in altra parola\n",
    "                all_hypon = list(s.closure(hypon, depth=10))  # TODO: aumentare a 2,3\n",
    "                hyponyms.extend([x.name().split(\".\")[0] for x in all_hypon])\n",
    "#                 print(\"SYN: {} \\t HYPER: {}\".format(s,t))\n",
    "\n",
    "#             print(index, hyponyms, \"\\n\")\n",
    "#         else:\n",
    "#             print(\"NADA\")\n",
    "            \n",
    "        \n",
    "        hyponyms_list.append(' '.join(hyponyms))\n",
    "    \n",
    "#     print(hyponyms_list)\n",
    "\n",
    "        \n",
    "    '''\n",
    "    CountVectorizer will create k vectors in n-dimensional space, where:\n",
    "    - k is the number of sentences,\n",
    "    - n is the number of unique words in all sentences combined.\n",
    "    If a sentence contains a certain word, the value will be 1 and 0 otherwise\n",
    "    '''\n",
    "    \n",
    "    vectorizer = CountVectorizer()\n",
    "    matrix = vectorizer.fit_transform(hyponyms_list)\n",
    "    \n",
    "    feature_list = vectorizer.get_feature_names()\n",
    "    vectors = matrix.toarray()\n",
    "    \n",
    "    m = vectors.sum(axis=0).argmax()\n",
    "    \n",
    "    print(m)\n",
    "    print(feature_list[m] + '\\n')\n",
    "#     print(feature_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
